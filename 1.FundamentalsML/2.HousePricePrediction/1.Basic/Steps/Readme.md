<h1 align="center"  style="margin-bottom: -10px;"> House Price Prediction with Machine Learning </h1>
<div align="center">

 This README is available in: [English](README.md) | [Spanish](README.es.md)

</div>

<h2 align="center"> Table of Contents</h2>

1. [Introduction](#1-introduction)
2. [Development Process](#2-development-process)
   - [1.1 Data Processing](#21-data-processing)
   - [1.2 Exploratory Data Analysis (EDA)](#22-exploratory-data-analysis-eda)
   - [1.3 Training Multiple Algorithms](#23-training-multiple-algorithms)
   - [1.4 Evaluation Metrics](#24-evaluation-metrics)
   - [1.5 Optimization (Tuning & Hyperparameters)](#25-optimization-tuning--hyperparameters)
   - [1.6 Manual Feature Engineering](#26-manual-feature-engineering)
   - [1.7 Data Augmentation (Tabular)](#27-data-augmentation-tabular)
3. [Final Results](#3-final-results)
4. [Technologies Used](#4-technologies-used)
5. [How to Run](#5-how-to-run)


<h2 align="center">1.  Introduction </h2>

Este es un proyecto b谩sico de regresi贸n en el que se implementa una pipeline completa de Machine Learning (excepto la etapa de data augmentation) para predecir el precio de una vivienda. El modelo toma en cuenta caracter铆sticas clave como la calidad, el tama帽o y la ubicaci贸n, utilizando datos hist贸ricos del mercado inmobiliario.

A continuaci贸n, se explicar谩 cada paso para mostrar la l贸gica con la que se resolvi贸 el problema.

<h2 align="center">2.Development Process</h2>

<table style="width: 100%; table-layout: fixed;">
  <tr>
    <td style="width: 50%; vertical-align: top; text-align: left; padding-right: 20px;">
      El proceso de desarrollo de este proyecto sigue una estructura l贸gica en etapas que permiten construir un modelo de regresi贸n robusto y comprensible. <br>
      Comenzamos con la carga y limpieza de los datos, seguido de un an谩lisis exploratorio para entender mejor las variables y su impacto en el precio. <br>
      Luego, se entrenan m煤ltiples algoritmos para comparar su rendimiento utilizando m茅tricas adecuadas. <br>
      Para la evaluaci贸n, utilizamos estas m茅tricas con el fin de comparar y seleccionar el mejor algoritmo. <br>
      Despu茅s, optimizamos el modelo elegido para obtener mejores resultados y predicciones. <br>
      Posteriormente, integramos todo el flujo en un 煤nico programa robusto. <br>
      Finalmente, realizamos predicciones utilizando el modelo entrenado y presentamos los resultados en un dashboard interactivo.
    </td>
    <td style="width: 50%;">
      <img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/spanish.png?raw=true" alt="Dashboard Preview" style="width: 100%; height: auto;" />
    </td>
  </tr>
</table>

<h3 align="center">2.1 Procesamiento de Datos</h3>

<h4 align="center">2.1.1 Carga de datos</h4>
En el primer paso del proyecto, se carg贸 el conjunto de datos Ames Housing, obteniendo un total de 2,930 filas y 82 columnas.
Esto proporciona una base rica y detallada de caracter铆sticas que describen las propiedades, incluyendo aspectos como tama帽o, calidad, ubicaci贸n y m谩s.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/1.1.1.jpeg?raw=true" alt="Dashboard Preview" style="width: 100%; height: auto;" />

>[!NOTE]
>Esta etapa es crucial para tener una visi贸n general del dataset, identificar posibles errores o datos faltantes, y planificar los siguientes pasos de limpieza y an谩lisis.


<h4 align="center">2.1.2 Verificar los null values</h4>

Verificamos los valores nulos presentes en el dataset. En esta etapa, solo se realiza una inspecci贸n visual para entender c贸mo est谩 compuesta nuestra base de datos, sin tomar a煤n decisiones sobre qu茅 variables eliminar o conservar. Esto nos permite tener una mejor idea de la calidad y completitud de los datos antes de continuar con el an谩lisis.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/1.2.jpeg?raw=true" alt="Dashboard Preview" style="width: 100%; height: auto;" />


<h4 align="center">2.1.3 Identificaci贸n de datos duplicados y an谩lisis de tipos de datos</h4>

En este paso combinamos dos tareas importantes: primero, detectamos los datos duplicados para garantizar la calidad del dataset; luego, analizamos los tipos de datos presentes. Esta 煤ltima acci贸n es fundamental para planificar futuros procesos, ya que cada variable puede requerir un tratamiento diferente seg煤n su tipo.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/1.3.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">



<h3 align="center">2.2 Exploratory Data Analysis</h3>

<h4 align="center">2.2.1 Heatmap</h4>
Primero, realizamos un heatmap de toda la base de datos para visualizar qu茅 variables presentan relaciones entre s铆. Sin embargo, debido a la gran cantidad de datos, el gr谩fico no permite una visualizaci贸n clara. Por ello, en el siguiente paso filtramos y enfocamos el an谩lisis en las variables m谩s relevantes para obtener una mejor interpretaci贸n.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.1.1.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">

Despu茅s, se imprimen las variables m谩s relevantes que se utilizar谩n para generar un heatmap reducido. El objetivo es obtener una mejor visualizaci贸n y comprensi贸n de las relaciones entre las variables m谩s influyentes en este caso espec铆fico.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.1.2.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">

Por 煤ltimo, utilizamos las variables m谩s relevantes para generar un heatmap reducido, en el cual se observa que las dos primeras variables presentan la mayor correlaci贸n (mayor intensidad de color). Esta observaci贸n es 煤til, ya que en los siguientes pasos estas variables pueden ser modificadas o utilizadas para mejorar el modelo.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.1.3.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">

<h4 align="center">2.2.2 Pairplot</h4>

En este paso se utiliza un pairplot para visualizar el comportamiento de las variables seleccionadas. Ya que hemos identificado dos variables con alta correlaci贸n, es importante observar su distribuci贸n y relaci贸n visualmente para entender mejor c贸mo interact煤an entre s铆.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.1.4.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">

<h4 align="center">2.2.3 Estad铆sticas descriptivas</h4>

En base a las variables seleccionadas (Gr Liv Area y Overall Qual), obtenemos informaci贸n m谩s detallada sobre sus relaciones con el resto del dataset. Sin embargo, es importante recordar que SalePrice es nuestra variable objetivo (y), ya que nuestro prop贸sito principal es predecir su comportamiento.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.1.5.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">

<h4 align="center">2.2.4 Histogramas</h4>

Este paso nos ayuda a visualizar la distribuci贸n de los datos de cada variable. A trav茅s de los histogramas podemos:

- Detectar la forma de la distribuci贸n (normal, sesgada, etc.).
- Identificar posibles sesgos y valores at铆picos (outliers).
- Evaluar si alguna variable requiere una transformaci贸n (como logaritmos o escalado).
- Observar la distribuci贸n general entre variables num茅ricas.
- Tomar decisiones sobre el tipo de preprocesamiento que podr铆a mejorar el rendimiento del modelo.

Esta visualizaci贸n es clave para entender mejor nuestros datos antes de construir modelos predictivos.

|Distribuci贸n de Gr Liv Area|Distribuci贸n de SalePrice|Distribution of Overall Quality|
|------------------------|------------------------|-------------------| 
|<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.2.1.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">|<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.2.2.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">|<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.2.3.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> |

>[!IMPORTANT]
>A continuaci贸n veremos el comportamiento y las combinaciones de cada una en caso de seleccionarlo, pero aun no lo escogemos

|SalePrice and Gr Liv Area|SalePrice and Overall Qual|Sale Price, Gr Liv Area, overall qual |
|------------------------|------------------------|-------------------| 
|Ambas variables presentan una distribuci贸n sesgada y contienen valores at铆picos (outliers) extremos. Por ello, es recomendable utilizar MinMaxScaler, ya que es m谩s seguro para mantener la escala original sin verse afectado por dichos valores at铆picos. <br> Por otro lado, tambi茅n se podr铆a usar StandardScaler, pero hay que considerar que los outliers pueden influir significativamente en la media y la desviaci贸n est谩ndar, afectando el escalado.| SalePrice es una variable continua, mientras que Overall Qual es una variable ordinal con valores de 1 a 10 que representan la calidad. <br> No es necesario normalizar Overall Qual, ya que es un n煤mero discreto con significado espec铆fico. Por lo tanto, se podr铆a aplicar escalado 煤nicamente a SalePrice usando MinMaxScaler o StandardScaler.| MinMaxScaler es una opci贸n segura si queremos que todas las variables est茅n en el rango [0, 1]. <br> StandardScaler puede ser 煤til si Gr Liv Area y SalePrice siguen una distribuci贸n normal. <br> Por otro lado, Overall Qual es una variable ordinal, por lo que es recomendable mantenerla sin escalar para conservar su significado.|

<h4 align="center">2.2.5 Boxplot</h4>
Un boxplot es una representaci贸n gr谩fica que muestra la distribuci贸n de una variable num茅rica, destacando su mediana, cuartiles y posibles valores at铆picos (outliers).<br>
Permite identificar la dispersi贸n, simetr铆a y la presencia de datos extremos de forma r谩pida y visual.<br><br>

<h5 align="center">2.2.5.1 Boxplot SalePrice</h5>
Observamos varios puntos por encima del rango intercuart铆lico, lo que indica la presencia de valores at铆picos elevados. Esto sugiere que existen viviendas con precios considerablemente m谩s altos que el promedio del dataset.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.3.1.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"><br>

<h5 align="center">2.2.5.2 Boxplot Gr Liv Area</h5>

El boxplot de Gr Liv Area tambi茅n muestra la presencia de outliers elevados, con los bigotes superiores extendi茅ndose m谩s all谩 que en otros casos, lo que indica que algunas viviendas tienen 谩reas habitables significativamente mayores al rango t铆pico.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.3.2.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"><br>

<h5 align="center">2.2.5.3 Boxplot OverallQual</h5>

El boxplot de OverallQual presenta valores concentrados principalmente por debajo de la mediana, con pocos o ning煤n outlier visible, lo que refleja que la mayor铆a de las viviendas tienen una calidad general dentro de un rango m谩s limitado.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.3.3.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> <br>

Finalmente, analizamos el valor p (p-value) para evaluar la significancia estad铆stica de nuestras variables con respecto a la variable objetivo (SalePrice).

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.3.4.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

<h4 align="center">2.2.6 Distribuci贸n de los datos</h4>

Necesitamos calcular el skewness (coeficiente de sesgo) de cada variable para cuantificar el sesgo en su distribuci贸n, lo cual es importante para decidir qu茅 acciones tomar en el preprocesamiento.
Despu茅s de calcular el skewness, observamos que las variables Gr Liv Area y Overall Qual presentan un sesgo positivo, es decir, la mayor铆a de los valores son bajos, pero existen algunos valores muy altos que desv铆an la distribuci贸n.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.3.5.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

Despu茅s, filtramos las filas donde alguna variable tenga un valor menor o igual a 0, ya que en datos reales ese tipo de valores no deber铆an existir (por ejemplo, una superficie o precio negativo no tiene sentido). Esta verificaci贸n nos ayuda a detectar errores o inconsistencias en los datos antes de entrenar el modelo.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.3.6.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

Por 煤ltimo, pero no menos importante, volvemos a verificar los valores nulos, pero esta vez solo en las variables que realmente nos interesan para el modelo. Esto nos permite enfocar el preprocesamiento en las columnas relevantes y tomar decisiones informadas sobre c贸mo manejar los datos faltantes

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.3.7.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

<h4 align="center">2.2.7 Aplicamos logaritmos</h4>
Como pudimos observar, los datos presentan distribuciones sesgadas, lo cual puede afectar el rendimiento de los modelos de regresi贸n.
Para corregir este problema, aplicamos una transformaci贸n logar铆tmica, lo que nos permite reducir el sesgo y acercar la distribuci贸n a una forma m谩s sim茅trica. A continuaci贸n, se muestra una comparaci贸n antes y despu茅s de aplicar el logaritmo:

| Antes del logaritmo | Despu茅s del logaritmo |
|----------------------------------------------|--------------|
|<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.4.1.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;">|<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.4.2.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> |

<h4 align="center">2.2.8 Normalizamos los datos</h4>
Antes de finalizar el an谩lisis de distribuci贸n, es importante considerar que la variabilidad entre escalas tambi茅n puede afectar el rendimiento del modelo.
Por ello, es necesario normalizar los datos para que las variables tengan valores en rangos similares. Esto ayuda a que el modelo aprenda de manera m谩s eficiente y equitativa.
En este paso, observamos los valores de las variables seleccionadas para decidir qu茅 tipo de escalado aplicar.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.4.1.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

Por 煤ltimo, realizamos una gr谩fica para verificar si, tras aplicar la normalizaci贸n y corregir el sesgo, la distribuci贸n de los datos se ha ajustado adecuadamente.
Esta visualizaci贸n nos permite confirmar si la variable se aproxima a una distribuci贸n normal (forma de campana), lo cual es deseable para muchos algoritmos de Machine Learning.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/2.4.2.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 


<h3 align="center">2.3 Training Multiple Algorithms</h3>

Pasamos a la etapa de entrenamiento del modelo, donde aplicaremos diferentes algoritmos de regresi贸n para comparar su rendimiento.
Dado que se trata de un problema de regresi贸n, se entrenar谩n los siguientes modelos:

- KNN Regressor
- SVR (Support Vector Regressor)
- Redes Neuronales (MLP Regressor)
- Ridge Regression (Regularizaci贸n L2)
- Lasso Regression (Regularizaci贸n L1)
- LightGBM
- XGBoost

El objetivo de este paso es evaluar qu茅 modelo se adapta mejor a los datos en funci贸n de las m茅tricas de desempe帽o, interpretabilidad y complejidad computacional.

<h4 align="center">2.3.1 KNN Regressor</h4>

KNN Regressor predice el valor de un punto tomando el promedio de los k vecinos m谩s cercanos seg煤n una m茅trica de distancia. Es un modelo basado en instancias, sin entrenamiento real, ideal cuando se espera que datos similares tengan valores de salida similares.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/3.Training/3.1.1KNNRegressor.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

<h4 align="center">2.3.2 SVR(Support Vector Regressor)</h4>

SVR es una extensi贸n del algoritmo de Support Vector Machine para tareas de regresi贸n. Busca ajustar una l铆nea (o hiperplano) que prediga los datos con un margen de tolerancia definido, minimizando los errores fuera de ese margen. Es 煤til para datos no lineales y ofrece buena generalizaci贸n.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/3.Training/3.1.2SVR.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

<h4 align="center">2.3.3 Redes Neuronales (MLP)</h4>

El MLP Regressor (Perceptr贸n Multicapa) es una red neuronal con una o m谩s capas ocultas que aprende patrones complejos mediante propagaci贸n hacia adelante y retropropagaci贸n. Es ideal para capturar relaciones no lineales entre las variables y se adapta bien a conjuntos de datos con m煤ltiples caracter铆sticas.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/3.Training/3.1.3MLPRegressor.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

<h4 align="center">2.3.4 LightGBM</h4>
LightGBM es un algoritmo de Gradient Boosting optimizado para velocidad y eficiencia. Construye 谩rboles de decisi贸n de forma hoja a hoja (leaf-wise) en lugar de nivel por nivel, lo que mejora el rendimiento y precisi贸n. Es ideal para grandes vol煤menes de datos y tareas de regresi贸n con alta dimensionalidad.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/3.Training/3.1.4LightGBM.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 
<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/3.Training/3.1.4LightGBM2.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

<h4 align="center">2.3.5 Ridge Regression (L2 Regularization)</h4>
Ridge Regression es una extensi贸n de la regresi贸n lineal que aplica regularizaci贸n L2 para reducir el sobreajuste. Penaliza los coeficientes grandes al agregar su suma cuadrada al t茅rmino de p茅rdida, lo que estabiliza el modelo cuando hay multicolinealidad o muchas variables.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/3.Training/3.1.5RidgeRegression.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

<h4 align="center">2.3.6 Lasso Regression (L1 Regularization)</h4>
Lasso Regression utiliza regularizaci贸n L1, que penaliza la suma absoluta de los coeficientes. Esto no solo reduce el sobreajuste, sino que tambi茅n puede eliminar variables irrelevantes, ya que tiende a llevar algunos coeficientes exactamente a cero, funcionando como una forma de selecci贸n de variables.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/3.Training/3.1.6LassoREgressor.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 

<h4 align="center">2.3.7 XGBoost</h4>

XGBoost (Extreme Gradient Boosting) es un algoritmo de gradient boosting altamente optimizado y eficiente. Utiliza t茅cnicas avanzadas como regularizaci贸n, poda de 谩rboles, y paralelizaci贸n para mejorar tanto la precisi贸n como el rendimiento computacional. Es muy popular en competencias de Machine Learning por su capacidad de manejar datos complejos y ruidosos.

<img src="https://github.com/KevinAlberto01/3.MachineLearning/blob/main/1.FundamentalsML/2.HousePricePrediction/1.Basic/Steps/Img/3.Training/3.1.7XGBoost.jpeg?raw=true" alt="Tipos de datos" style="width: 100%; height: auto;"> 


<h3 align="center">2.4 Evaluation Metrics</h3>

<h3 align="center">2.5 Optimization (Tuning & Hyperparameters)</h3>

<h2 align="center">3.Final Results</h2>
<h2 align="center">4.Technologies Used</h2>
<h2 align="center">讹 How to Run</h2>

```bash
git clone https://github.com/yourusername/housing-ml-project.git
cd housing-ml-project
pip install -r requirements.txt
streamlit run streamlit_app.py
